{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a36b9a3",
   "metadata": {},
   "source": [
    "# LSTM English-to-French"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f219bf47",
   "metadata": {},
   "source": [
    "## I. Build the vocab before training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7446293",
   "metadata": {},
   "source": [
    "### Step 1: Prepare the dataset & tokenize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49550908",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "\n",
    "# Tokenizers\n",
    "en_tokenizer = get_tokenizer(\"spacy\", language=\"en_core_web_sm\")\n",
    "fr_tokenizer = get_tokenizer(\"spacy\", language=\"fr_core_news_sm\")\n",
    "\n",
    "SPECIAL_TOKENS = [\"<unk>\", \"<pad>\", \"<bos>\", \"<eos>\"]\n",
    "MAX_VOCAB = 10_004\n",
    "UNK_TOKEN = \"<unk>\"\n",
    "\n",
    "def get_tokens(path, tokenizer):\n",
    "    all_tokens = []\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            all_tokens.append(tokenizer(line.strip()))\n",
    "    return all_tokens\n",
    "\n",
    "enToken = get_tokens(\"./Data/train.en\", en_tokenizer)\n",
    "frToken = get_tokens(\"./Data/train.fr\", fr_tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6a07d61",
   "metadata": {},
   "source": [
    "### Step 2: Build the vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74a871e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English vocab size: 10004\n",
      "French vocab size: 10004\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from torchtext.vocab import build_vocab_from_iterator\n",
    "\n",
    "# ==== Build vocab function ====\n",
    "\n",
    "def yield_tokens(token_list):\n",
    "    for tokens in token_list:\n",
    "        yield tokens\n",
    "\n",
    "# ==== English vocab ====\n",
    "\n",
    "en_vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(enToken),\n",
    "    specials=SPECIAL_TOKENS,\n",
    "    max_tokens=MAX_VOCAB\n",
    ")\n",
    "\n",
    "en_vocab.set_default_index(en_vocab[UNK_TOKEN])\n",
    "\n",
    "# ==== French vocab ====\n",
    "\n",
    "fr_vocab = build_vocab_from_iterator(\n",
    "    yield_tokens(frToken),\n",
    "    specials=SPECIAL_TOKENS,\n",
    "    max_tokens=MAX_VOCAB\n",
    ")\n",
    "\n",
    "fr_vocab.set_default_index(fr_vocab[UNK_TOKEN])\n",
    "\n",
    "# ==== Check vocab size ====\n",
    "print(\"English vocab size:\", len(en_vocab))\n",
    "print(\"French vocab size:\", len(fr_vocab))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec78430",
   "metadata": {},
   "source": [
    "Finally, we have built two vocabularies for the two languages. Each vocabulary contains the 10,000 most frequent words in the dataset, along with four special tokens: <`unk`>, <`pad`>, <`sos`>, and <`eos`>."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2815fc6",
   "metadata": {},
   "source": [
    "## II. Padding & Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9273011c",
   "metadata": {},
   "source": [
    "### Step 1: Sync the lenght of batch with pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9a59225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def collate_fn(sentences, vocab, pad_token='<pad>'):\n",
    "    \"\"\"\n",
    "    sentences: list các câu (string hoặc list token)\n",
    "    vocab: torchtext vocab object\n",
    "    pad_token: token dùng để pad\n",
    "    \"\"\"\n",
    "    pad_idx = vocab[pad_token]\n",
    "    \n",
    "    # 1️⃣ Nếu input là string, bạn có thể tokenize ở đây\n",
    "    # Giả sử sentences đã là list token, nếu là string thì tokenize tùy cách bạn muốn\n",
    "    # sentences = [sentence.split() for sentence in sentences]\n",
    "    \n",
    "    # 2️⃣ Chuyển token sang index\n",
    "    batch_indices = [torch.tensor([vocab[token] for token in sentence]) \n",
    "                     for sentence in sentences]\n",
    "    \n",
    "    # 3️⃣ Lấy độ dài trước khi pad\n",
    "    lengths = [len(seq) for seq in batch_indices]\n",
    "    \n",
    "    # 4️⃣ Pad tất cả về cùng chiều dài\n",
    "    src_padded = pad_sequence(batch_indices, batch_first=True, padding_value=pad_idx)\n",
    "    \n",
    "    return src_padded, lengths\n",
    "\n",
    "# collate_fn(,en_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8515f9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# packed = pack_padded_sequence(padded, lengths, batch_first=True, enforce_sorted=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
